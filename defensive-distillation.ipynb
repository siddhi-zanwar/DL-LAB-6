{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets","metadata":{"_uuid":"0ee7ded8-624c-4bc4-a1cf-57b17247fb07","_cell_guid":"bc6bc9da-2110-4c99-8232-1bc0e11ac300","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-09T10:20:29.390891Z","iopub.execute_input":"2025-03-09T10:20:29.391074Z","iopub.status.idle":"2025-03-09T10:20:36.026704Z","shell.execute_reply.started":"2025-03-09T10:20:29.391056Z","shell.execute_reply":"2025-03-09T10:20:36.026017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n\n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return torch.relu(out)\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = ResidualBlock(64, 128, stride=2)\n        self.layer2 = ResidualBlock(128, 256, stride=2)\n        self.layer3 = ResidualBlock(256, 512, stride=2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avg_pool(x)\n        x = torch.flatten(x, 1)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:23:03.015356Z","iopub.execute_input":"2025-03-09T10:23:03.015697Z","iopub.status.idle":"2025-03-09T10:23:03.024763Z","shell.execute_reply.started":"2025-03-09T10:23:03.015676Z","shell.execute_reply":"2025-03-09T10:23:03.023754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def softmax_with_temperature(logits, T):\n    \"\"\"Apply softmax with temperature scaling.\"\"\"\n    return F.softmax(logits / T, dim=1)\n\nclass DistillationLoss(nn.Module):\n    \"\"\"Knowledge Distillation Loss using KL Divergence.\"\"\"\n    def __init__(self, T):\n        super(DistillationLoss, self).__init__()\n        self.T = T\n\n    def forward(self, student_logits, teacher_probs):\n        student_probs = F.log_softmax(student_logits / self.T, dim=1)\n        return F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\") * (self.T ** 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ntrain_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nteacher_model = CNN().to(device)\noptimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 30\nprint(\"Training Teacher Model...\")\nfor epoch in range(num_epochs):\n    teacher_model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = teacher_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\ntorch.save(teacher_model.state_dict(), \"/kaggle/working/teacher_model.pth\")\nprint(\"Teacher Model Trained & Saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"T = 5.0  # High temperature for soft labels\nteacher_model.eval()\n\nsoft_labels_list = []\nwith torch.no_grad():\n    for inputs, _ in train_loader:\n        inputs = inputs.to(device)\n        logits = teacher_model(inputs)\n        soft_labels_list.append(softmax_with_temperature(logits, T))\n\nsoft_labels = torch.cat(soft_labels_list)  # Stack all soft labels\nprint(\"Soft Labels Generated.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_model = CNN().to(device)\noptimizer = optim.Adam(student_model.parameters(), lr=0.001)\ncriterion = DistillationLoss(T)\n\nprint(\"Training Student Model...\")\nfor epoch in range(num_epochs):\n    student_model.train()\n    for (inputs, _), soft_targets in zip(train_loader, soft_labels):\n        inputs, soft_targets = inputs.to(device), soft_targets.to(device)\n        optimizer.zero_grad()\n        student_logits = student_model(inputs)\n        loss = criterion(student_logits, soft_targets)\n        loss.backward()\n        optimizer.step()\n\ntorch.save(student_model.state_dict(), \"/kaggle/working/student_model.pth\")\nprint(\"Student Model Trained & Saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}